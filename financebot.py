# ç¦ç”Ÿæ— é‡å¤©å°Š
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
import traceback

# OpenAI API Key
openai_api_key = os.getenv("OPENAI_API_KEY")
# ä»ç¯å¢ƒå˜é‡è·å– Serveré…± SendKeys
server_chan_keys_env = os.getenv("SERVER_CHAN_KEYS")
if not server_chan_keys_env:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")
server_chan_keys = server_chan_keys_env.split(",")

openai_client = OpenAI(api_key=openai_api_key, base_url="https://api.deepseek.com/v1")

# RSSæºåœ°å€åˆ—è¡¨
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»":{
        "åå°”è¡—è§é—»":"https://dedicated.wallstreetcn.com/rss.xml",      
    },
    "ğŸ’» 36æ°ª":{
        "36æ°ª":"https://36kr.com/feed",   
        },
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±":"https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ":"http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹":"http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘":"https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ":"https://www.stats.gov.cn/sj/zxfb/rss.xml",
    },
      "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
        "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº":"https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}

# é…ç½®å¸¸é‡
TIMEOUT_SECONDS = 10  # æ¯ç¯‡æ–‡ç« è¯·æ±‚è¶…æ—¶ 10s
REQUEST_RETRIES = 3   # RSS è¯·æ±‚ä¸ç½‘ç»œè¯·æ±‚çš„çŸ­é‡è¯•æ¬¡æ•°
REQUEST_RETRY_DELAY = 2  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"


# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

def safe_referer_from_url(url: str) -> str:
    try:
        from urllib.parse import urlparse
        p = urlparse(url)
        if p.scheme and p.netloc:
            return f"{p.scheme}://{p.netloc}/"
    except Exception:
        pass
    return ""

# çˆ¬å–ç½‘é¡µæ­£æ–‡ (ç”¨äº AI åˆ†æï¼Œä½†ä¸å±•ç¤º)
# ä¿®æ”¹è¯´æ˜ï¼š
# - ä½¿ç”¨ requests.get(..., timeout=TIMEOUT_SECONDS) è·å– HTMLï¼Œé¿å… newspaper.download() çš„å†…éƒ¨ç½‘ç»œè°ƒç”¨æ— è¶…æ—¶æ§åˆ¶
# - è¿”å› (text, error)ï¼Œtext å…è®¸ä¸ºç©ºï¼›error ä¸º None è¡¨ç¤ºæˆåŠŸï¼Œå¦åˆ™ä¸ºé”™è¯¯æè¿°å­—ç¬¦ä¸²
def fetch_article_text(url):
    headers = {
        'User-Agent': USER_AGENT,
        'Referer': safe_referer_from_url(url),
        'Accept-Language': 'zh-CN,zh;q=0.9,en-US,en;q=0.8'
    }

    # ç®€å•é‡è¯•é€»è¾‘ï¼ˆé’ˆå¯¹ç¬æ—¶ç½‘ç»œé”™è¯¯ï¼‰
    last_err = None
    for attempt in range(REQUEST_RETRIES):
        try:
            print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url} (attempt {attempt+1})")
            resp = requests.get(url, headers=headers, timeout=TIMEOUT_SECONDS)
            status = resp.status_code
            if status != 200:
                # 403/401/å…¶ä»–è§†ä¸ºå¤±è´¥ï¼Œè¿”å›ç©ºæ–‡æœ¬å¹¶è®°å½•é”™è¯¯
                err_msg = f"HTTP_{status}"
                print(f"âŒ HTTP çŠ¶æ€é200: {status}ï¼ŒURL: {url}")
                return "", err_msg
            html = resp.text or ""
            if not html.strip():
                print(f"âš ï¸ æ–‡ç« HTMLä¸ºç©º: {url}")
                # è¿”å›ç©ºæ–‡æœ¬ï¼Œä½†ä¸ç®—é”™è¯¯
                return "", None
            # ç”¨ newspaper è§£æ HTMLï¼ˆé¿å…å†æ¬¡å‘èµ·ç½‘ç»œè¯·æ±‚ï¼‰
            try:
                article = Article(url)
                article.set_html(html)
                article.parse()
                text = (article.text or "")[:1500]  # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡º API è¾“å…¥é™åˆ¶
                if not text:
                    print(f"âš ï¸ æ–‡ç« è§£æåæ­£æ–‡ä¸ºç©º: {url}")
                return text, None
            except Exception as parse_exc:
                # è§£æå¤±è´¥ï¼šè®°å½•é”™è¯¯ï¼Œä½†è¿”å›ç©ºæ–‡æœ¬ä»¥ä¾¿ç»§ç»­æ•´ä½“æµç¨‹
                err = f"parse_error: {parse_exc}"
                print(f"âŒ è§£ææ–‡ç« å¤±è´¥: {url}, é”™è¯¯: {parse_exc}")
                return "", err
        except requests.Timeout:
            # è¶…æ—¶ç«‹å³è§†ä¸ºè¯¥æ–‡ç« çˆ¬å–å¤±è´¥å¹¶è·³è¿‡ï¼ˆæ ¹æ®éœ€æ±‚ï¼‰
            print(f"âŒ è¶…æ—¶ ({TIMEOUT_SECONDS}s) åœ¨ URL: {url}")
            return "", "timeout"
        except requests.RequestException as req_exc:
            last_err = req_exc
            print(f"âš ï¸ è¯·æ±‚å¼‚å¸¸: {req_exc}ï¼ŒURL: {url}ï¼Œ{REQUEST_RETRIES-attempt-1} æ¬¡é‡è¯•å‰©ä½™")
            time.sleep(REQUEST_RETRY_DELAY)
            continue
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯åœ¨çˆ¬å– URL {url}: {traceback.format_exc()}")
            return "", f"unknown_error: {e}"
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    return "", f"request_exception: {last_err}"

# æ·»åŠ  User-Agent å¤´
def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': USER_AGENT
    }
    # feedparser.parse æ”¯æŒ request_headers å‚æ•°
    return feedparser.parse(url, request_headers=headers)


# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
            else:
                print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} æœªè·å–åˆ°æ¡ç›®")
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
        time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None

# è·å–RSSå†…å®¹ï¼ˆçˆ¬å–æ­£æ–‡ä½†ä¸å±•ç¤ºï¼‰
# ä¿®æ”¹ï¼šæ”¶é›† failures åˆ—è¡¨ï¼›fetch_article_text è¿”å› (text, error)ï¼›å³ä¾¿å­˜åœ¨é”™è¯¯ä¹Ÿç»§ç»­å¤„ç†å·²çˆ¬å–å†…å®¹
def fetch_rss_articles(rss_feeds, max_articles=10):
    news_data = {}
    analysis_text = ""  # ç”¨äºAIåˆ†æçš„æ­£æ–‡å†…å®¹
    failures = []  # è®°å½•çˆ¬å–å¤±è´¥çš„æ¡ç›®ä¿¡æ¯

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, url in sources.items():
            print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")
            feed = fetch_feed_with_retry(url)
            if not feed:
                print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                failures.append({"source": source, "url": url, "error": "rss_fetch_failed"})
                continue
            print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

            articles = []  # æ¯ä¸ª source éƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–åˆ—è¡¨
            count = 0
            for entry in feed.entries:
                if count >= max_articles:
                    break
                title = entry.get('title', 'æ— æ ‡é¢˜')
                link = entry.get('link', '') or entry.get('guid', '')
                if not link:
                    print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                    failures.append({"source": source, "title": title, "url": "", "error": "no_link"})
                    continue

                # çˆ¬å–æ­£æ–‡ç”¨äºåˆ†æï¼ˆä¸å±•ç¤ºï¼‰
                text, err = fetch_article_text(link)
                if err is None:
                    # æˆåŠŸæˆ–è§£æä¸ºç©ºä½†æ²¡æœ‰é”™è¯¯
                    if text:
                        analysis_text += f"ã€{title}ã€‘\n{text}\n\n"
                else:
                    # è®°å½•å¤±è´¥ä¿¡æ¯ï¼›å¦‚æœ text é‡Œæœ‰å†…å®¹ï¼ˆä¾‹å¦‚è§£æéƒ¨åˆ†æˆåŠŸï¼‰ï¼Œä»åŠ å…¥åˆ†æ
                    failures.append({"source": source, "title": title, "url": link, "error": err})
                    if text:
                        analysis_text += f"ã€{title}ã€‘\n{text}\n\n"

                print(f"ğŸ”¹ {source} - {title} å¤„ç†å®Œæ¯• (url: {link})")
                articles.append(f"- [{title}]({link})")
                count += 1

            if articles:
                category_content += f"### {source}\n" + "\n".join(articles) + "\n\n"

        news_data[category] = category_content

    return news_data, analysis_text, failures

# AI ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆåŸºäºçˆ¬å–çš„æ­£æ–‡ï¼‰
def summarize(text):
    # è‹¥æ²¡æœ‰æ­£æ–‡å¯ä»¥åˆ†æï¼Œè¿”å›å ä½æ–‡æœ¬å¹¶ä¸è°ƒç”¨APIï¼ˆé¿å…è°ƒç”¨ç©ºå†…å®¹ï¼‰
    if not text or not text.strip():
        return "ï¼ˆæœªèƒ½è·å–åˆ°è¶³å¤Ÿçš„æ­£æ–‡ç”¨äºè‡ªåŠ¨åˆ†æï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹çš„çˆ¬å–ç»“æœä¸å¤±è´¥åˆ—è¡¨ã€‚ï¼‰"

    try:
        completion = openai_client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": """
                 ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼š
                 1. æå–æ–°é—»ä¸­æ¶‰åŠçš„ä¸»è¦è¡Œä¸šå’Œä¸»é¢˜ï¼Œæ‰¾å‡ºè¿‘1å¤©æ¶¨å¹…æœ€é«˜çš„3ä¸ªè¡Œä¸šæˆ–ä¸»é¢˜ï¼Œä»¥åŠè¿‘3å¤©æ¶¨å¹…è¾ƒé«˜ä¸”æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡çš„3ä¸ªè¡Œä¸š/ä¸»é¢˜ã€‚ï¼ˆå¦‚æ–°é—»æœªæä¾›å…·ä½“æ¶¨å¹…ï¼Œè¯·ç»“åˆæè¿°å’Œå¸‚åœºæƒ…ç»ªæ¨æµ‹çƒ­ç‚¹ï¼‰
                 2. é’ˆå¯¹æ¯ä¸ªçƒ­ç‚¹ï¼Œè¾“å‡ºï¼š
                    - å‚¬åŒ–å‰‚ï¼šåˆ†æè¿‘æœŸä¸Šæ¶¨çš„å¯èƒ½åŸå› ï¼ˆæ”¿ç­–ã€æ•°æ®ã€äº‹ä»¶ã€æƒ…ç»ªç­‰ï¼‰ã€‚
                    - å¤ç›˜ï¼šæ¢³ç†è¿‡å»3ä¸ªæœˆè¯¥è¡Œä¸š/ä¸»é¢˜çš„æ ¸å¿ƒé€»è¾‘ã€å…³é”®åŠ¨æ€ä¸é˜¶æ®µæ€§èµ°åŠ¿ã€‚
                    - å±•æœ›ï¼šåˆ¤æ–­è¯¥çƒ­ç‚¹æ˜¯çŸ­æœŸç‚’ä½œè¿˜æ˜¯æœ‰æŒç»­è¡Œæƒ…æ½œåŠ›ã€‚
                 3. å°†ä»¥ä¸Šåˆ†ææ•´åˆä¸ºä¸€ç¯‡1500å­—ä»¥å†…çš„è´¢ç»çƒ­ç‚¹æ‘˜è¦ï¼Œé€»è¾‘æ¸…æ™°ã€é‡ç‚¹çªå‡ºï¼Œé€‚åˆä¸“ä¸šæŠ•èµ„è€…é˜…è¯»ã€‚
                 4. æ ¹æ®è¿™äº›ä¿¡æ¯åˆ†æç›¸å…³æœ€å¤§å—ç›ŠAè‚¡ä¸ªè‚¡å‰10ä½ï¼Œç»™å‡ºè‚¡ç¥¨åç§°ï¼Œä»£ç åŠåˆ©å¥½åˆ†æã€‚
                 """},
                {"role": "user", "content": text}
            ]
        )
        # deepseek è¿”å›å¯èƒ½ä¸ openai ä¸å®Œå…¨ä¸€è‡´ï¼Œä¿å®ˆå¤„ç†
        # åœ¨å¤§å¤šæ•° SDK ä¸­ï¼Œresponse.choices[0].message.content å¯ç”¨
        choice = completion.choices[0]
        # å…¼å®¹ä¸åŒ response ç»“æ„
        if hasattr(choice, "message") and hasattr(choice.message, "content"):
            return choice.message.content.strip()
        elif isinstance(choice, dict) and "message" in choice and "content" in choice["message"]:
            return choice["message"]["content"].strip()
        elif hasattr(choice, "text"):
            return choice.text.strip()
        else:
            return str(choice)
    except Exception as e:
        # è®°å½•ä½†ä¸æŠ›å‡ºï¼Œè¿”å›å ä½æ–‡æœ¬ä»¥ä¾¿æµç¨‹ç»§ç»­
        print(f"âŒ è°ƒç”¨ deepseek/OpenAI ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}\n{traceback.format_exc()}")
        return "ï¼ˆè‡ªåŠ¨åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œæœªèƒ½ç”Ÿæˆæ‘˜è¦ï¼›è¯·æŸ¥çœ‹åŸæ–‡é“¾æ¥ä¸çˆ¬å–å¤±è´¥åˆ—è¡¨ä»¥è·å–è¯¦æƒ…ã€‚ï¼‰"

# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    for key in server_chan_keys:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        try:
            response = requests.post(url, data=data, timeout=10)
            if response.ok:
                print(f"âœ… æ¨é€æˆåŠŸ: {key}")
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.status_code} {response.text}")
        except Exception as e:
            print(f"âŒ å‘é€åˆ° server é…±å¤±è´¥: {e}")


def send_to_feishu(webhooks, title, content):
    # webhooks: list[str]
    for url in webhooks:
        payload = {
            "msg_type": "markdown",
            "markdown": {"title": title, "text": content}
        }
        try:
            resp = requests.post(url, json=payload, timeout=10)
            if resp.ok:
                print(f"âœ… Feishu æ¨é€æˆåŠŸ: {url}")
            else:
                print(f"âŒ Feishu æ¨é€å¤±è´¥: {url}, å“åº”ï¼š{resp.status_code} {resp.text}")
        except Exception as fe:
            print(f"âŒ å‘é€åˆ° Feishu å¤±è´¥: {fe}")


def chunk_text_by_len(text, max_len=2000):
    lines = text.splitlines(True)
    chunks = []
    cur = ""
    for line in lines:
        if len(cur) + len(line) > max_len:
            if cur:
                chunks.append(cur)
                cur = line
            else:
                for i in range(0, len(line), max_len):
                    chunks.append(line[i:i+max_len])
                cur = ""
        else:
            cur += line
    if cur:
        chunks.append(cur)
    return chunks


if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")

    # æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š 10 ç¯‡æ–‡ç« ï¼ˆå¯è°ƒæ•´ï¼‰
    articles_data, analysis_text, failures = fetch_rss_articles(rss_feeds, max_articles=10)
    
    # AIç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœåˆ†ææ­£æ–‡ä¸ºç©ºï¼Œsummarize ä¼šè¿”å›å ä½æ–‡æœ¬ï¼‰
    summary = summarize(analysis_text)

    # ç”Ÿæˆä»…å±•ç¤ºæ ‡é¢˜å’Œé“¾æ¥çš„æœ€ç»ˆæ¶ˆæ¯
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"

    # åœ¨æ¶ˆæ¯å°¾éƒ¨è¿½åŠ å¤±è´¥æ‘˜è¦ï¼ˆåŒ…å« URL + é”™è¯¯ä¿¡æ¯ï¼‰
    if failures:
        final_summary += "\n\n---\n\nçˆ¬å–å¤±è´¥åˆ—è¡¨ï¼ˆè‹¥æœ‰å¤šæ¡ï¼ŒæŒ‰é¡ºåºåˆ—å‡ºï¼‰ï¼š\n"
        for f in failures:
            src = f.get("source", "")
            title = f.get("title", "")
            url = f.get("url", "")
            err = f.get("error", "")
            if title:
                final_summary += f"- [{title}]({url}) ({src}): {err}\n"
            else:
                final_summary += f"- {url} ({src}): {err}\n"

    # æ¨é€åˆ°å¤šä¸ª server é…± key
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)

    # Feishu æ¨é€ï¼ˆå¤š webhookï¼‰
    feishu_env = os.getenv("FEISHU_WEBHOOK_URLS")
    webhooks = []
    if feishu_env:
        webhooks = [u.strip() for u in feishu_env.split(",") if u.strip()]
    else:
        single = os.getenv("FEISHU_WEBHOOK_URL")
        if single:
            single = single.strip()
            if single:
                webhooks = [single]
    if webhooks:
        for category, content in articles_data.items():
            if not content.strip():
                continue
            full_text = f"### {category}\n{content}"
            chunks = chunk_text_by_len(full_text, max_len=2000)
            total = len(chunks)
            for idx, part in enumerate(chunks, start=1):
                title = f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦ - {category} (Part {idx}/{total})"
                send_to_feishu(webhooks, title, part)
